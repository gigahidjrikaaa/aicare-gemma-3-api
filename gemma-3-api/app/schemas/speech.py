"""Pydantic schemas for speech-related endpoints."""

from __future__ import annotations

from typing import List, Optional

from pydantic import AliasChoices, BaseModel, ConfigDict, Field, PositiveInt


class SpeechTranscriptionSegment(BaseModel):
    id: Optional[int] = Field(default=None, description="Segment identifier supplied by Whisper.")
    start: Optional[float] = Field(default=None, description="Start timestamp in seconds.")
    end: Optional[float] = Field(default=None, description="End timestamp in seconds.")
    text: str = Field(..., description="Transcribed text for the segment.")


class SpeechTranscriptionResponse(BaseModel):
    text: str = Field(..., description="Full transcript returned by the STT backend.")
    language: Optional[str] = Field(default=None, description="Detected or requested language code.")
    segments: List[SpeechTranscriptionSegment] = Field(
        default_factory=list,
        description="Optional list of timestamped segments.",
    )


class SpeechTranscriptionOptions(BaseModel):
    language: Optional[str] = Field(default=None, description="Preferred language hint.")
    prompt: Optional[str] = Field(default=None, description="Optional priming text for Whisper.")
    response_format: Optional[str] = Field(
        default=None,
        description="Override the Whisper response format (json, verbose_json, srt, etc.).",
    )
    temperature: Optional[float] = Field(
        default=None,
        ge=0.0,
        le=1.0,
        description="Sampling temperature forwarded to Whisper.",
    )


class SpeechSynthesisRequest(BaseModel):
    model_config = ConfigDict(populate_by_name=True)

    text: str = Field(
        ...,
        description="Plain text that should be synthesised.",
        validation_alias=AliasChoices("text", "input"),
    )
    voice: Optional[str] = Field(default=None, description="Voice preset to use during synthesis.")
    model: Optional[str] = Field(default=None, description="Specific Higgs Audio model to target.")
    response_format: Optional[str] = Field(
        default=None,
        description="Desired audio container/codec (pcm, wav, mp3, ogg, flac).",
    )
    sample_rate: Optional[PositiveInt] = Field(
        default=None,
        description="Target audio sample rate in Hz.",
    )
    speed: Optional[float] = Field(
        default=None,
        gt=0.0,
        description="Optional playback speed multiplier.",
    )
    style: Optional[str] = Field(default=None, description="Optional style or emotion preset.")
    stream: bool = Field(
        default=False,
        description="When true, stream the audio response rather than returning base64.",
    )


class SpeechSynthesisResponse(BaseModel):
    audio_base64: str = Field(..., description="Base64 encoded audio payload.")
    response_format: str = Field(..., description="Audio format returned by the TTS backend.")
    media_type: str = Field(..., description="MIME type corresponding to the audio payload.")
    sample_rate: PositiveInt = Field(..., description="Sample rate of the audio in Hz.")
    voice: str = Field(..., description="Voice preset used during synthesis.")
    model: str = Field(..., description="Model identifier that produced the speech.")


class SpeechDialogueResponse(BaseModel):
    transcript: SpeechTranscriptionResponse = Field(
        ..., description="Transcription metadata returned by Whisper."
    )
    response_text: str = Field(..., description="Text response generated by the LLM.")
    audio_base64: str = Field(..., description="Base64 encoded audio payload from Higgs Audio.")
    response_format: str = Field(..., description="Audio format returned by the synthesis backend.")
    media_type: str = Field(..., description="MIME type corresponding to the audio payload.")
    sample_rate: PositiveInt = Field(..., description="Sample rate of the audio in Hz.")
    voice: str = Field(..., description="Voice preset used during synthesis.")
    model: str = Field(..., description="Model identifier that produced the speech.")
