# Gemma 3 API with WebSocket Streaming

This project provides a FastAPI-based API for serving the Gemma 3 language model, with support for real-time streaming via WebSockets.

## WebSocket Streaming Endpoint

The primary endpoint for streaming text generation is:

**URL**: `ws://<your_server_address>:6666/v1/generate_ws`

### How it Works

The WebSocket connection allows for low-latency, stateful, and bidirectional communication. Once a connection is established, the client can send generation requests, and the server will stream back the model's response token by token.

### Input Message Format

To start a generation, the client must send a JSON message with the following structure:

```json
{
  "prompt": "Your text prompt here",
  "max_tokens": 512,
  "temperature": 0.7
}
```

- **`prompt`** (str, required): The initial text to start the generation from.
- **`max_tokens`** (int, optional, default: `512`): The maximum number of tokens to generate.
- **`temperature`** (float, optional, default: `0.7`): The sampling temperature. Higher values make the output more random.

### Output Message Format

The server will send back a series of JSON messages for each generation request.

**1. Token Stream**

For each token generated by the model, the server sends a message like this:

```json
{
  "token": "a"
}
```

```json
{
  "token": " generated"
}
```

```json
{
  "token": " token"
}
```

The client should concatenate the values of the `"token"` key to reconstruct the full response.

**2. End of Stream**

Once the generation is complete (either by reaching the `max_tokens` limit or by the model generating a stop sequence), the server will send a final message to signal completion:

```json
{
  "status": "done"
}
```

When the client receives this message, it knows that the current stream has ended and can finalize the result.

## Example Client Implementation (Python)

Here is a simple Python client demonstrating how to connect to the endpoint, send a request, and handle the streamed response.

```python
import asyncio
import websockets
import json

async def main():
    """
    Connects to the WebSocket, sends a prompt, and prints the streamed response.
    """
    uri = "ws://localhost:6666/v1/generate_ws"
    
    try:
        async with websockets.connect(uri) as websocket:
            # 1. Define the generation request
            prompt_data = {
                "prompt": "Write a short story about a robot who discovers music.",
                "max_tokens": 100,
                "temperature": 0.8
            }
            
            # 2. Send the request to the server
            await websocket.send(json.dumps(prompt_data))
            print(f"Sent prompt: '{prompt_data['prompt']}'")
            print("--- Server Response ---")

            # 3. Handle the streamed response
            full_response = ""
            while True:
                response = await websocket.recv()
                data = json.loads(response)
                
                # If the 'status' key is present, the stream is done
                if data.get("status") == "done":
                    print("\n--- End of Stream ---")
                    break
                
                # Otherwise, it's a token
                token = data.get("token", "")
                full_response += token
                print(token, end="", flush=True)

    except websockets.exceptions.ConnectionClosed as e:
        print(f"Connection closed unexpectedly: {e}")
    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    asyncio.run(main())

```

### How to Run the Example

1.  Ensure the main application is running in Docker.
2.  Save the code above as a Python file (e.g., `test_client.py`).
3.  Run the script from your terminal: `python test_client.py`
